'''
블로그 url이 저장파일 --> 블로그별 본문 --> 파일에 저장
'''


import requests
from bs4 import BeautifulSoup
import time

SEARCH_WORDS= ['설화수']

def make_input_file(word):
    '''
    네이버 검색결과(블로그 url)가 저장된 파일명을 지정한다
    '''
    inputfile="./naverblogs/NaverSearchResult_"+word+".txt"
    
    return inputfile


def make_output_file(word):
    '''
    블로그별 본문내용을 저장할 파일명을 지정한다
    '''
    outputfile="./naverblogs/blogs_texts_"+word+".txt"
    
    return outputfile



def get_html(url):
    '''
    url 주소 넣으면 html을 리턴한다
    '''
    user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) " +\
        "AppleWebKit/537.36 (KHTML, like Gecko) " + \
        "Chrome/37.0.2062.94 Safari/537.36"
    headers = {"User-Agent": user_agent}
    response=requests.get(url, headers=headers)
    html=response.text
    
    return html


def get_texts(url):
    '''
    url 주소를 넣으면 블로그 본문을 읽어준다
    '''
    
    html_url=get_html(url)
    soup=BeautifulSoup(html_url,"lxml")
    texts=""
    for products in soup.find_all("div",attrs={'class':'post_ct'}):
        texts+=products.get_text().replace("\n"," ").replace("\u200b"," ").replace("\xa0"," ")
    
    return texts

    
for word in SEARCH_WORDS:
    inputfile=make_input_file(word)
    texts=""

    with open(inputfile, "r", encoding="utf-8") as input_file:
        for url in input_file:
            text=get_texts(url)
            texts+=text
            time.sleep(3)
            print(word,url,len(text)) #현재진도점검용
        time.sleep(5)
         
    outputfile=make_output_file(word)
    with open(outputfile, "w", encoding="utf-8") as output_file:
        print(texts, file=output_file)
        

    
