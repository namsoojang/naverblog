'''
naver blogs 크롤링(PC)
'''

import requests
from bs4 import BeautifulSoup
import time

word='애견샴푸'  #sample
PAGENUM_START=1
PAGENUM_END=1    # 한 페이지당 블로그 10개씩 조회
FILE_URLS='./data/url.txt'

def make_search_pageurl(word, page_no):
    '''
    검색어와 검색 페이지 번호 입력 --> 해당 검색결과 페이지 url     
    '''    
    pageurl='https://search.naver.com/search.naver?where=post&sm=tab_pge&query=%s\
            &st=sim&date_option=0&date_from=&date_to=&dup_remove=1&post_blogurl=&post_blogurl_without=&\
            srchby=all&nso=&ie=utf8&start=%d'%(word, page_no*10+1)
    
    return pageurl


def get_html(url):
    '''
    url 주소 넣으면 html을 리턴한다
    '''
    user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) " +\
        "AppleWebKit/537.36 (KHTML, like Gecko) " + \
        "Chrome/37.0.2062.94 Safari/537.36"
    headers = {"User-Agent": user_agent}
    response=requests.get(url, headers=headers)
    html=response.text
    
    return html
   


def get_blogs_url(word):
    '''
    검색어 입력 --> 검색된 블로그 url 주소 
    select 활용
    '''
    blogs_urls=[]
    for pagenum in range(PAGENUM_START, PAGENUM_END+1):
        print(pagenum)
        url=make_search_pageurl(word, pagenum)
        html=get_html(url)
        soup=BeautifulSoup(html, 'lxml')
        urls=soup.select('#elThumbnailResultArea a.url')
        for url in urls:
            blogs_urls.append(url.string)
        time.sleep(1)
    return blogs_urls



urls=get_blogs_url(word)

with open(FILE_URLS, "w", encoding="utf-8") as output_file:
    for url in urls:
        print(url, file=output_file)
